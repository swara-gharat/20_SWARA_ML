# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14IdDRM0wIuDeqL3AK_6XXTNWJime8z9s

EXP 2 LOGISTIC REGRESSION
"""

import pandas as pd

df = pd.read_csv('Titanic-Dataset.csv')
df.head()

df.info()

df = df[['Survived', 'Age', 'Sex', 'Pclass']]
df = pd.get_dummies(df, columns=['Sex', 'Pclass'])
df.dropna(inplace=True)
df.head()

from sklearn.model_selection import train_test_split

x = df.drop('Survived', axis=1)
y = df['Survived']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=0)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(random_state=0)
model.fit(x_train, y_train)

model.score(x_test, y_test)

from sklearn.model_selection import cross_val_score

cross_val_score(model, x, y, cv=5).mean()

from sklearn.metrics import confusion_matrix

y_predicted = model.predict(x_test)
confusion_matrix(y_test, y_predicted)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(model, x_test, y_test)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_predicted))

from sklearn.metrics import RocCurveDisplay

RocCurveDisplay.from_estimator(model, x_test, y_test)

female = [[30, 1, 0, 1, 0, 0]]
model.predict(female)[0]

male = [[60, 0, 1, 0, 0, 1]]
probability = model.predict_proba(male)[0][1]
print(f'Probability of survival: {probability:.1%}')

probability = model.predict_proba(female)[0][1]
print(f'Probability of survival: {probability:.1%}')

"""EXP 4 RANDOM FOREST

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
df = pd.read_csv('adult.csv')
print(df.head())

df.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain', 'capital-loss','hours-per-week', 'native-country', 'income']
df.replace(' ?', pd.NA, inplace=True)
df.dropna(inplace=True)

df.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain', 'capital-loss','hours-per-week', 'native-country', 'income']
df.replace(' ?', pd.NA, inplace=True)
df.dropna(inplace=True)
categorical_columns = ['workclass', 'education', 'marital-status','occupation','relationship', 'race', 'sex', 'native-country', 'income']
label_encoders = {}
for col in categorical_columns:
  le = LabelEncoder()
df[col] = le.fit_transform(df[col])
label_encoders[col] = le
print(df)

X = df.drop('income', axis=1)
y = df['income']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)
X_train, X_test = X_train.align(X_test, join='left', axis=1)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

from sklearn.tree import export_graphviz
import pydotplus
from IPython.display import Image, display
y_pred = rf.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"ClassificationReport:\n{classification_report(y_test,y_pred)}")

"""EXP 6 HIERARCHICAL CLUSTERING

"""

#Hierarchical Clustering, ,
import os
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import normalize
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering

data = pd.read_csv('Wholesale customers data.csv')
print(data.head())

data_scaled = normalize(data)
data_scaled = pd.DataFrame(data_scaled, columns=data.columns)
print(data_scaled.head())

plt.figure(figsize=(10, 7))
plt.title("Dendrograms")
d = shc.dendrogram(shc.linkage(data_scaled, method='ward'))

plt.figure(figsize=(10, 7))
plt.title("Dendrograms")
d = shc.dendrogram(shc.linkage(data_scaled, method='ward'))
plt.axhline(y=6, color='r', linestyle='--')

cluster = AgglomerativeClustering(n_clusters=2, linkage='ward')
print(cluster.fit_predict(data_scaled))

plt.figure(figsize=(10, 7))
plt.scatter(data_scaled['Milk'], data_scaled['Grocery'], c=cluster.labels_)



"""EXP 1 LINEAR REGRESSION"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

Boston=pd.read_csv('BostonHousing.csv')
Boston.head()

Boston.info()
Boston.describe()

missing_values = Boston.isna().sum()
print(missing_values)

na_columns = ['rm']
Boston[na_columns] = Boston[na_columns].fillna(Boston.mean())
print(Boston)

target = Boston['medv']
print(target)

sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.distplot(Boston['medv'], bins=30)
plt.show()

correlation_matrix = Boston.corr().round(2)
# annot = True to print the values inside the square
sns.heatmap(data=correlation_matrix, annot=True)

plt.figure(figsize=(20, 5))

features = ['lstat', 'rm']
target = Boston['medv']

for i, col in enumerate(features):
    plt.subplot(1, len(features) , i+1)
    x = Boston[col]
    y = target
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('medv')

X = pd.DataFrame(np.c_[Boston['lstat'], Boston['rm']], columns = ['lstat','rm'])
Y = Boston['medv']

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lin_model = LinearRegression()
lin_model.fit(X_train, Y_train)

# model evaluation for training set
from sklearn.metrics import r2_score
y_train_predict = lin_model.predict(X_train)
rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))
r2 = r2_score(Y_train, y_train_predict)

print("The model performance for training set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))
print("\n")

# model evaluation for testing set
y_test_predict = lin_model.predict(X_test)
rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))
r2 = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

"""EXP 7 DIMENSIONALITY REDUCTION"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

df = pd.read_csv('adult.csv', header=None)

columns = ['age', 'workclass', 'fnlwgt', 'education', 'education.num',
           'marital.status', 'occupation', 'relationship', 'race',
           'sex', 'capital.gain', 'capital.loss', 'hours.per.week',
           'native.country', 'income']
df.columns = columns
df

# Handle missing values (if any) - removing or filling them

df = df.replace(' ?', np.nan)
df.dropna(inplace=True)

# Encode categorical features
label_encoders = {}
for column in ['workclass', 'education', 'marital.status', 'occupation',
               'relationship', 'race', 'sex', 'native.country', 'income']:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le
df

df = df.drop(index=1)

# Reset the index if you want a continuous index after dropping
df.reset_index(drop=True, inplace=True)
print(df)



X = df.drop('income', axis=1)
y = df['income']

print(X.dtypes)

imputer = SimpleImputer(strategy='mean')
X_imputer = imputer.fit_transform(X)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputer)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

pca = PCA(n_components=5)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""EXP 5 BOOSTING ALGORITHM"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
df = pd.read_csv('adult.csv')
print(df.head())

columns = ['age', 'workclass', 'fnlwgt', 'education', 'education.num',
           'marital.status', 'occupation', 'relationship', 'race',
           'sex', 'capital.gain', 'capital.loss', 'hours.per.week',
           'native.country', 'income']
df.columns = columns

df = df.replace(' ?', np.nan)
df.dropna(inplace=True)

label_encoders = {}
for column in ['workclass', 'education', 'marital.status', 'occupation',
               'relationship', 'race', 'sex', 'native.country', 'income']:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le
df

X = df.drop('income', axis=1)
y = df['income']

def bootstrap_evaluate(X,y,n_boootstrap=100):
  accuracies=[]
  for i in range(n_bootstraps):
    indices = np.random.randint(0,len(X),len(X))

    bootstrap_X, bootstrap_y = X.iloc[indices],y.iloc[indices],y.iloc[indices]

    x_train, x_test, y_train, y_test = train_test_split(bootstrap_X, bootstrap_y, test_size=0.2, random_state=42)

    rf = RandomForestClassifier(n_estimators=100, random_state= 42)

    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

  mean_accuracy = np.mean(accuracies)
  std_accuracy = np.std(accuracies)

  return mean_accuracy, std_accuracy

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X and y are already defined
# mean_acc, std_acc = bootstrap_evaluate(X, y, n_bootstraps=30)

X_train, X_test_final, y_train_final, y_test_final = train_test_split(X, y, test_size=0.2, random_state=42)

rf_final = RandomForestClassifier(n_estimators=100, random_state=42)
rf_final.fit(X_train, y_train_final)

y_pred_final = rf_final.predict(X_test_final)

print(f"Final model evaluation:")
print(f"Accuracy: {accuracy_score(y_test_final, y_pred_final)}")

print(f"classification report:\n{classification_report(y_test_final, y_pred_final)}")
# print(f"mean accuracy (bootstraping) : {mean_accuracy}")
# print(f"standard deviation of accuracy (bootsraping) : {std_accuracy}")

"""EXP 3 DECISION TREE"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Adult dataset path
adult_dataset_path = "adult.csv"

# Function for loading adult dataset
def load_adult_data(adult_path=adult_dataset_path):
    csv_path = os.path.join(adult_path)
    return pd.read_csv(csv_path)

# Calling load adult function and assigning to a new variable df
df = load_adult_data()
# load top 3 rows values from adult dataset
df.head(3)

print ("Rows     : " ,df.shape[0])
print ("Columns  : " ,df.shape[1])
print ("\nFeatures : \n" ,df.columns.tolist())
print ("\nMissing values :  ", df.isnull().sum().values.sum())
print ("\nUnique values :  \n",df.nunique())

df.info()
df.describe()

df_missing = (df=='?').sum()
df_missing

df.apply(lambda x: x !='?',axis=1).sum()

# select all categorical variables
df_categorical = df.select_dtypes(include=['object'])

# checking whether any other column contains '?' value
df_categorical.apply(lambda x: x=='?',axis=1).sum()

# dropping the "?"s from occupation and native.country
df = df[df['occupation'] !='?']
df = df[df['native.country'] !='?']

from sklearn import preprocessing

# encode categorical variables using label Encoder

# select all categorical variables
df_categorical = df.select_dtypes(include=['object'])
df_categorical.head()

# apply label encoder to df_categorical
le = preprocessing.LabelEncoder()
df_categorical = df_categorical.apply(le.fit_transform)
df_categorical.head()

# first, Drop earlier duplicate columns which had categorical values
df = df.drop(df_categorical.columns,axis=1)
df = pd.concat([df,df_categorical],axis=1)
df.head()

# convert target variable income to categorical
df['income'] = df['income'].astype('category')

# Importing train_test_split
from sklearn.model_selection import train_test_split
# Putting independent variables/features to X
X = df.drop('income',axis=1)

# Putting response/dependent variable/feature to y
y = df['income']
X.head(3)

# Splitting the data into train and test
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=99)

X_train.head()

# Importing decision tree classifier from sklearn library
from sklearn.tree import DecisionTreeClassifier

# Fitting the decision tree with default hyperparameters, apart from
# max_depth which is 5 so that we can plot and read the tree.
dt_default = DecisionTreeClassifier(max_depth=5)
dt_default.fit(X_train,y_train)

# Importing classification report and confusion matrix from sklearn metrics
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

# making predictions
y_pred_default = dt_default.predict(X_test)

# Printing classifier report after prediction
print(classification_report(y_test,y_pred_default))

# Printing confusion matrix and accuracy
print(confusion_matrix(y_test,y_pred_default))
print(accuracy_score(y_test,y_pred_default))

